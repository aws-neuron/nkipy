{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP NKIPy Tutorial\n",
    "\n",
    "This tutorial uses a simple Multi-Layer Perceptron (MLP) NKIPy kernel to demonstrate how NKIPy works with more complex neural network operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nkipy.core.trace import NKIPyKernel\n",
    "from nkipy.runtime.execute import baremetal_run_traced_kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining A Simple MLP NKIPy Kernel\n",
    "\n",
    "A simple MLP consists of:\n",
    "1. A linear transformation (matrix multiplication + bias)\n",
    "2. An activation function (SiLU/Swish)\n",
    "3. Another linear transformation\n",
    "\n",
    "This is a basic two-layer feedforward network using SiLU activation, which is commonly used in modern neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silu_kernel(x):\n",
    "    \"\"\"SiLU (Swish) activation function: x * sigmoid(x).\"\"\"\n",
    "    return x * (1 / (1 + np.exp(-x)))\n",
    "\n",
    "\n",
    "def mlp_kernel(x, weight1, bias1, weight2, bias2):\n",
    "    \"\"\"Simple MLP with two linear layers and SiLU activation.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor [batch_size, input_dim]\n",
    "        weight1: First layer weight [input_dim, hidden_dim]\n",
    "        bias1: First layer bias [hidden_dim]\n",
    "        weight2: Second layer weight [hidden_dim, output_dim]\n",
    "        bias2: Second layer bias [output_dim]\n",
    "\n",
    "    Returns:\n",
    "        Output tensor [batch_size, output_dim]\n",
    "    \"\"\"\n",
    "    # First linear layer\n",
    "    hidden = np.matmul(x, weight1) + bias1\n",
    "\n",
    "    # SiLU (Swish) activation\n",
    "    hidden_activated = silu_kernel(hidden)\n",
    "\n",
    "    # Second linear layer\n",
    "    output = np.matmul(hidden_activated, weight2) + bias2\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the MLP Kernel as a NumPy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 2048)\n",
      "Weight1 shape: (2048, 8192)\n",
      "Bias1 shape: (8192,)\n",
      "Weight2 shape: (8192, 2048)\n",
      "Bias2 shape: (2048,)\n",
      "\n",
      "NumPy output shape: (2, 2048)\n",
      "NumPy output range: [20279.5098, 21601.3789]\n"
     ]
    }
   ],
   "source": [
    "# Create test data\n",
    "batch_size = 2\n",
    "input_dim = 2048\n",
    "hidden_dim = 8192\n",
    "output_dim = 2048\n",
    "\n",
    "# Input data\n",
    "x = np.random.rand(batch_size, input_dim).astype(np.float32)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Network parameters\n",
    "weight1 = np.random.rand(input_dim, hidden_dim).astype(np.float32) * 0.1\n",
    "bias1 = np.random.rand(hidden_dim).astype(np.float32) * 0.1\n",
    "weight2 = np.random.rand(hidden_dim, output_dim).astype(np.float32) * 0.1\n",
    "bias2 = np.random.rand(output_dim).astype(np.float32) * 0.1\n",
    "\n",
    "print(f\"Weight1 shape: {weight1.shape}\")\n",
    "print(f\"Bias1 shape: {bias1.shape}\")\n",
    "print(f\"Weight2 shape: {weight2.shape}\")\n",
    "print(f\"Bias2 shape: {bias2.shape}\")\n",
    "\n",
    "# Run as NumPy function\n",
    "out_numpy = mlp_kernel(x, weight1, bias1, weight2, bias2)\n",
    "print(f\"\\nNumPy output shape: {out_numpy.shape}\")\n",
    "print(f\"NumPy output range: [{np.min(out_numpy):.4f}, {np.max(out_numpy):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing the MLP Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the kernel\n",
    "traced_kernel = NKIPyKernel.trace(mlp_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running it On Trainium Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baremetal output shape: (2, 2048)\n",
      "Baremetal output range: [20279.5078, 21601.3789]\n",
      "Is the baremetal output the same as NumPy? True\n"
     ]
    }
   ],
   "source": [
    "# Run on Trainium hardware\n",
    "out_baremetal = baremetal_run_traced_kernel(\n",
    "    traced_kernel, x, weight1, bias1, weight2, bias2\n",
    ")\n",
    "print(f\"Baremetal output shape: {out_baremetal.shape}\")\n",
    "print(\n",
    "    f\"Baremetal output range: [{np.min(out_baremetal):.4f}, {np.max(out_baremetal):.4f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"Is the baremetal output the same as NumPy? {np.allclose(out_baremetal, out_numpy)}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
